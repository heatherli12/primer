# Priorities

## Students

Nuo Wen 8/16
Kevin 8/19
Arghayan 9/3
Yuan 9/4
Heather 9/20
Atilla 9/25
Ajay 1/1
Ryan 1/1
Anmay 1/1

Heather (advanced plots and case studies) and Kevin (basic plots) -- chapter 1
Ajay (data types), Atilla (plots) -- chapter 2
Anmay -- chapter 3
Mak (rewrite), Sophia (shaming), Arghayan (tables) -- chapter 4
Yuhan  -- chapter 5
Ryan -- chapter 6
George -- chapter 7
Anmay  -- tutorials
Nuo Wen -- infrastructure

## Important topics for this summer

* We need a "machine" which generates these predictions, which is the same thing as a machine which fills in all the question marks in the Actual Preceptor Table, which is the same thing as a machine which produces "fake data" which looks a lot like our actual data. This also leads directly to the concept of *posterior predictive checks*, which is just fancy terminology for helping to see if your model makes sense. If your model is reasonable, then you would expect to see Z (a feature of the real data) in either new data or in fake data generated by your model. If you see Z, then you should have more faith in your model. If you don't, then something is wrong.

* Get rid of 90% of the parameter interpretations discussions. Parameters, much less the posterior distributions of parameters, are not that interesting. We don't care with complex models, so we should not care (much) with simple models. Got a substantive question? Ask it directly as a newobs.

* Recall the two major hacks in our use of rstanarm. First, if there is only a constant in the model, we need to use `newdata = tibble(constant = 1)`. Better (and perhaps now available in development version) would be `newdata = tibble(.rows = 1)`, after showing students what `tibble(.rows = 1)` produces. Second, there `posterior_predict()` requires `as_tibble()` and `mutate_all(numeric)`. The first is OK. I don't object to explicitly changing matrices to tibbles. But the second is absurd, although necessary to deal with the columns being weird ppd class. This is also fixed in the dev version. With luck, we can get rid of these soon because a new version comes out. Requiring the installation of the dev version makes me nervous because of compilation issues.

* Related to the above is the latest version of the posteriors package and the new rvars data type. This looks like good stuff that will form the skeleton of Bayesian analysis going forward. But do we want to discuss this in the Primer? Maybe? Con is that it is too much. Just let students use tidybayes are some similar high level approach. Pro is that random variables are a key concept. The rvars allows you to do some fun stuff, stuff which reinforces the conceptual knowledge we want students to have. Also makes certain things, like manipulating draws to calculating causal effects much (?) easier.

* Maybe the best approach is to see to experiment a bit with the new version of tidybayes . . . Should also consider switching away from rstanarm and to brms, which seems more actively maintained. But I am very nervous that because "brms is based on Stan, a C++ compiler is required." But I really like the categorical() family in brms. Why doesn't rstanarm have something similar? It is in stan itself: https://mc-stan.org/docs/2_27/stan-users-guide/multi-logit-section.html. 

* https://github.com/stan-dev/rstanarm/issues/540

* At one point, I noticed that we got different answers in Chapter 6 between stan_glm() and creating the posterior by hand. I now think that this is a bug in rstanarm. See https://github.com/stan-dev/rstanarm/issues/509. Or maybe not! Looks like we use a bernoulli set up, so no problem?

* Add one or two other types of models. Maybe stan_gamm4(), for generalized additive models, or stan_polr(), for ordinal models. Or both? Idea is that the basic purpose of Justice is to choose a functional form. Given that, we ought to have a bunch of functional forms which we try. Sure wish that there were a stan_multilogit(), which would be an easy extention from logistic. (Maybe there is? I need to explore the different family arguments in stan_glm().) Something for time series might also be useful, given how common time series are in reality. Or maybe we can stick with stan_glm() and just use one of the other 6 or 7 family options. Maybe Gamma, which ensures that the outcome is positive. (Example: https://mc-stan.org/rstanarm/articles/continuous.html). Or poissson? Some of this might work well in chapter 8, since some of the left-hand side variables have few possible value and/or are always positive. multinomial/categorical regressions are available in the brms package. One way to still use rstanarm for this is discussed here: https://github.com/stan-dev/rstanarm/issues/20, trick is using Poisson count models.


* Keep in mind that the rvars package is coming along and will serve as the new foundation for the posterior package. This will change how we plot posteriors in the future, probably with a new version of tidybayes. Working with ggdist directly is not the best approach, I think.

* Discuss: Should "posterior probability distribution" become "ppd" or "PPD" everywhere?

## Other stuff

* Upgrade to new version of bookdown. And check out this book with R and Python mixed together. https://peopleanalytics-regression-book.org/. Also: https://bookdown.org/

* https://clauswilke.com/dataviz/

* Guide to the packages themselves, especially magic like Github Actions. Connect to this: https://www.rostrum.blog/2020/08/09/ghactions-pkgs/

* Consider this for entire document:
html_document:
    code_folding: "hide"




* Use chrome_print() to make PDF files instead of having to worry about tinytex and other arcana: https://rdrr.io/cran/pagedown/man/chrome_print.html

* When do we start using the new pipe? Whenever that is, we will want to update Getting Started to make the new pipe the default for CMD-shift-M. Presumably, there is a command for doing so. Discussion: https://blog.rstudio.com/2021/06/09/rstudio-v1-4-update-whats-new/


* Preceptor's Posterior is a posterior for which all the assumptions are true. Just because the big Data Science Machine has spat out a posterior does not mean that you should believe it blindly.

* Replace geom_histogram with ggdist throughout, although leave the first example and show how ggdist just makes everything easier.

* Look to this for motivation: https://mc-stan.org/rstanarm/articles/continuous.html

* Need to add fig.cap to any R code chunk for a figure which we want to reference. "If we assign a figure caption to a code chunk via the chunk option fig.cap, R plots will be put into figure environments, which will be automatically labeled and numbered, and can also be cross-referenced."

* Use ragg? ragg can be used when knitting Rmarkdown files by setting dev="ragg_png" in the code chunk options

https://www.tidyverse.org/blog/2021/02/modern-text-features/

* Add the notion of Preceptor's Posterior earlier.



* Stop spending time interpreting coefficients. Make fun of the practice. Instead, just ask a question and then answer it.

* $ git reset --soft HEAD~1

* The elipses should be vertical, not horizontal in all Preceptor Tables. fmt_markdown() would allow us to have vertical elipses in the Preceptor Tables.

* Discuss ps_2 to PS_2 problem on Github.

* Discuss relative paths and home directories more in Tools.

* Add grouping for line plots.

* Explaing age*gender versus age:gender distinction?

* Get rid of joins except left joins, and mention the others.

* Think about the difference between the *posterior distribution* of average height and the *posterior probability distribution*. Right now, we only use the latter. But the draws which are produced from epred() and predict() are really more examples of the former. Aren't they? That is, 4,000 height values is a *distribution*, as are all vectors, and it is *posterior* since we created it with a function that starts with "posterior". But nor is it a probability distribution since we have not normalized it yet. Shouldn't we weave this connection throughout the book.

* Should I use MAD_SD, MAD, mad or what? Should be consistent.

* Standardize use of "tidyverse". Only two choices: Either Tidyverse, when referring to the concept and/or to the collection of packages, or **tidyverse**, when referring to the package itself. Never use "tidyverse."

* Add ModernDive common problems isssues: https://moderndive.com/C-appendixC.html#data-wrangling

* https://leanpub.com/markua/

* https://education.rstudio.com/blog/2021/02/cbds/

* Start using "When comparing" everywhere.

* Fix tools to include discussion of Git for Windows and setting up the Terminal correctly: https://happygitwithr.com/shell.html#windows-shell-hell.

* Split maps into census and maps. Combine with ipums. Add code for making ipums graphics.

* Every model should feature a plot of predicted values and true outcomes. The decomposition is fine as far as it goes, but it is not the key image.

* Think harder about p() and Prob(). Which goes where?

* Use fitted() or predict() or both?

* Need testing in each chapter.

* Find/remove all usage of "controlling for x, we see". Use "adjust" instead.

* What do we think about the width of the code in the book? Sure seems like the comment lines go on for too long. Maybe? I don't like the way they care cut off in my Ipad, but . . . do many students read on Ipads?

* https://xkcd.com/2048/

* Show the secret weapon of doing the same regression many times and then gathering the results. nest_by(), perhaps.

* Better automated grading? https://ubc-dsci.github.io/rudaux

* Discuss the meaning of sigma more often. Really only need one sentence, but should give that sentence almost every time.

* Include links to disputes about governors work:

https://erikgahner.dk/2020/a-response-to-andrew-gelman/
https://statmodeling.stat.columbia.edu/2020/07/02/no-i-dont-believe-that-claim-based-on-regression-discontinuity-analysis-that/
https://github.com/jonspring/discontinuity/blob/master/fileb23c55435f90.gif

* Greek letters are for parameters which we can never observe. Latin letters are for observables.



* First, we can just print it. Chapter 7 walks the reader through all the parts of a stan_glm() model in detail. Later chapters will also show the printed model, but can move more quickly. Second, we create a summary table of the model using **gtsummary**. Note that this is just a different view of the same model. We don't show some things --- like sigma --- that we did show when just printing. We do show other things, like the 95% confidence interval which we did not show before. Neither is better! We use the one which is most helpful to our audience. Third, we use **tidybayes** to show histograms of the posterior distributions. The posterior is the underlying reality, the closest to the "truth" which we are going to get. The printed and table outputs are just summaries of the posterior. We might not show all three things every time, but we certainly always show the posterior. Graphics are pretty!

* After noting this formula, each example should create a plot with three histograms in a row --- left-to-right, the outcome (i.e., a histogram or density of Y), the fitted values (which is sometimes a spike, sometimes two spikes and so on) and, finally, the residuals. This highlights how we have *decomposed* the outcome into two parts: the model and the unmodeled variation. This belongs in the Courage chapter because it is a way of understanding the model we have made.



## To Discuss Later


* I like the hack of making your Rmd an index.Rmd and then the html will appear as a Github pages for free.


* Should use summary(fit_1) and discuss at some point. But when? Maybe Chapter 10?

* Do we need a cool graphics appendix, with  brief descriptions, pretty pictures from cool packages like: ggrepel, gghighlights, plotly. Others?



* More discussion of what it means to "control for" something in a regression. Right now, we don't mention this until chapter 11. Needs to be covered earlier, especially in chapter 9. That sets the stage for later.

* Standardize notation. What are predicted/fitted values? Use y_i everywhere, instead of using problem specific terms? That seems a bad (good?!) idea.

* Generate some error messages and then show that they can teach you something. Object does not exist. filter only have one equal sign, and so on. Do this all the time. Normalize the generation and processing of error messages.



*  Better workflow, automatic (with click confirmation) replacement of docs/ after successful check. Two parts: First, change the check so that it builds the book nicely. (Maybe not necessary. Maybe building the book in junk/ is OK?) Second, move the newly created book --- once it is accepted --- to docs/ on the master branch. Should we be taking multiple branches more seriously. Probably.
  + Things to discuss:
    - This change will prevent authors from easily knitting individual chapters. I will explore alternatives.
    - Two ways of handing auto-building and auot-updating:
      - 1) Create a new development branch. Students will submit PR to that branch. When you are ready, you can send a PR to merge to the master
           branch. The changes will only be live when you merge the development branch to the master branch.
      - 2) Keep things as they are. Once a student submits a PR, the build will directly go into docs/ and if successful, you can accept the PR
           and the new changes to the book will immediately be live. If the build check fails, do not accept the PR or else the deployment might
           break.


* Can you rebuild just a single chapter and then commit/push it? Right now, I have to rebuild the whole thing each time I want to make a single change. Takes too long.



# Appendices

Appendices have information that either a) a prof might reasonably decide not to assign or b) often contain material that students already know.

* Why Bayes?

* Messed up research articles. We need to prepare case studies of messed up articles. Start with those that Gelman cites. The tricky part is trying to figure out how to include these in class. And during which week do we use them.


* All the math you don't need to know. Bayes Theorem. Formulas. Normal distribution. Central limit theorem

* Advanced graphics, especially a tour of cool packages, including ggplotly and leaflet. gghighlight. ggstream.

* List of cool packages, googlesheet4 examples. : https://datavizm20.classes.andrewheiss.com/example/10-example/

* How to make an Rpubs and gists and saveWidgets:
https://datavizm20.classes.andrewheiss.com/example/10-example/

* Tufte and other graphics luminaries

* Leamer and other famous articles

* rtweet

* Making memes. Someone should figure out which meme maker is best for R. Or maybe we make our own. And then we make lots of memes for the book!


# From the Bookdown book

preview_chapter() and serve_book() as an aid to chapter writers.

webshot() tool for including images taken from webpages. Everytime we mention how cool source X is, we should provide a webshot of it. (And we should test that it exists.) Make the productivity chapter include way less of our prose.


p. 64 notes that adding the suffix 2 to various output formats gives you all the cool stuff, like figure captioning and numbering.

p. 74 has a useful discussion of configuration options for the _bookdown.yml file.

* rmd_subdir are subdirectories to search for source Rmd files. That seems critical for my submodule structure.

* output_dir is the output directory of the process (_book by default).

* clean is vector of files and directories to be cleaned by the clean_book() function.

pp. 5-6 discuss rmd_files as the way to define your own ordering for output files. This also goes in the _bookdown.yml file.


## Other links of interest:

https://r-charts.com/ (use these tutorials)
https://www.youtube.com/watch?v=CQS4xxz-2s4 (marginal and conditional distributions; quite hard; maybe optional?)


* Books

https://rstudio4edu.github.io/rstudio4edu-book/
https://ubc-dsci.github.io/introduction-to-datascience/
https://monika76five.github.io/ProbBayes/
https://bookdown.org/roback/bookdown-BeyondMLR/
https://dtkaplan.github.io/DataComputingEbook/

* Courses

https://ubc-dsci.github.io/dsci-100/
https://dataviz-2021.netlify.app/
https://jmbuhr.de/dataIntro20/
https://athanasiamo.github.io/tidyquintro/

* Other

https://evamaerey.github.io/flipbooks/about (use flipbooks for classroom exercises?)
https://nickch-k.github.io/EconometricsSlides/
https://holtzy.github.io/Pimp-my-rmd/

 https://fromthebottomoftheheap.net/2020/04/30/rendering-your-readme-with-github-actions/
 https://github.com/itsyaoyu/github_actions/blob/master/.github/workflows/main.yml
 https://www.willandskill.se/en/deleting-your-git-commit-history-without-removing-repo-on-github-bitbucket/

https://mdbeckman.github.io/JSM2020-Virtual/

* More cartoons like [xkcd](https://cran.r-project.org/package=RXKCD)

styler package

https://storywrangling.org/

https://desiree.rbind.io/post/2019/making-tip-boxes-with-bookdown-and-rmarkdown/ for making pretty boxes in the book

https://github.com/wikimedia/waxer for Wikipedia data

http://zevross.com/blog/2017/06/19/tips-and-tricks-for-working-with-images-and-figures-in-r-markdown-documents/

https://github.com/agmath/AppliedStatsInteractive

https://github.com/moodymudskipper/flow

https://github.com/ucbds-infra/ottr-sample

https://github.com/allisonhorst/stats-illustrations

https://bookdown.org/yihui/rmarkdown-cookbook/equatiomatic.html

Consider Netifly: https://cerebralmastication.com/2019/05/11/publishing-bookdown-to-netlify-automagically/

https://kieranhealy.org/categories/visualization/

https://openintro-ims.netlify.app/

https://education.rstudio.com/blog/2020/07/gtsummary/
https://moderndive.github.io/moderndive_labs/index.html
https://education.rstudio.com/blog/2020/07/learning-learnr/
https://rmarkdown.rstudio.com/authoring_shiny_prerendered.HTML

https://rstudio-education.github.io/tidyverse-cookbook/. I love this image: https://twitter.com/icymi_r/status/1407199200627113989/photo/1



* Look at the **flair** package to format the code. Or does that require us to have two copies of code: working copy and colored copy? https://education.rstudio.com/blog/2020/05/flair/

* Does using **flipbookr** make sense in the middle of a chapter?

* https://github.com/yonicd/carbonate -- perhaps useful for some nicer formatting of source code.


Take a look at drat.

https://community.rstudio.com/t/native-pipe-with-lm/110956 Pipe bend might be important. 

### Quotes to add:

https://blogs.bmj.com/bmj/2021/07/05/time-to-assume-that-health-research-is-fraudulent-until-proved-otherwise/ "It may be time to move from assuming that research has been honestly conducted and reported to assuming it to be untrustworthy until there is some evidence to the contrary." -- Richard Smith, editor of the BMJ for 13 years.



### Distribution Talk

A variable in a tibble is a column, a vector of values. We sometimes refer to this vector as a "distribution." This is somewhat sloppy in that a distribution can be many things, most commonly a mathematical formula. But, strictly speaking, a "frequency distribution" or an "empirical distribution" is a list of values, so this usage is not unreasonable.

*There are two distinct concepts: a distribution and a set values drawn from that distribution.*  But, in everyday use, we use "distribution" to both. When given a distribution (meaning a vector of numbers), we often use `geom_histogram()` or `geom_density()` to graph it. But, sometimes, we don't want to look at the whole thing. We just want some summary measures which report the key aspects of the distribution. The two most important attributes of a distribution are its *center* and its *variation* around that center.

We use `summarize()` to calculate statistics for a variable, a column, a vector of values or a distribution. Note the language sloppiness. For the purposes of this book, "variable," "column," "vector," and "distribution" all mean the same thing. Popular statistical functions include: `mean()`, `median()`, `min()`, `max()`, `n()` and `sum()`. Functions which may be new to you include three measures of the "spread" of a distribution: `sd()` (the standard deviation), `mad()` (the scaled median absolute deviation) and `quantile()`, which is used to calculate an *interval* which includes a specified proportion of the values. 

*A distribution is a function that shows the possible values of a variable and how often they occur.*

Think of the distribution of a variable as an urn from which we can pull out, at random, values for that variable. Drawing a thousand or so values from that urn, and then looking at a histogram, can show where the values are centered and how they vary. Because people are sloppy, they will use the word distribution to refer to both the (imaginary!) urn from which we are drawing values and to the list of values we have drawn. It is better, however, to keep three distinct ideas separate:

* The *unknown true distribution* which, in reality, generates the data which we see. Outside of stylized examples in which we *assume* that a distribution follows a simple mathematical formula, we will never have access to the unknown true distribution. We can only estimate it. This unknown true distribution is often referred to as the *data generating mechanism*, or DGM. It is a function or black box or urn which produces data. We can see the data. We can't see the urn. Later in the *Primer*, once we have learned about posterior distributions, we will often refer to this as *Preceptor's Posterior*.

* The *estimated distribution* which, we think, generates the data which we see. Again, we can never know the unknown true distribution. But, by making some assumptions and using the data we have, we can *estimate* a distribution. Our estimate may be very close to the true distribution. Or it may be far away. The main task of data science to to create and use these estimated distributions. Almost always, these distributions are instantiated in computer code.

* A *vector of numbers drawn* from the estimated distribution. Both true and estimated distributions can be complex beasties, difficult to describe accurately and in detail. But a vector of numbers drawn from a distribution is easy to understand and use. So, in general, we work with vectors of numbers. When someone --- either a colleague or a piece of R code --- creates a distribution which we want to use to answer a question, we don't really want the distribution itself. Rather, we want a vectors of "draws" from that distribution. Vectors are easy to work with! Complex computer code is not.

Again, people (including us!) will often be sloppy and use the same word, "distribution," without making it clear whether they are talking about the *true distribution*, the *estimated distribution*, or a vector of *draws* from the estimated distribution. Try not to be sloppy.

### Scaling a distribution

Consider the vector which is the result of rolling one die 10 times.

```{r}
rolls <- c(5, 5, 1, 5, 4, 2, 6, 2, 1, 5)
```

There are other ways of storing the data in this vector. Instead of recording every draw, we could just record the number of times each value appears.

```{r}
table(rolls)
```


In this case, with only 10 values, it is actually less efficient to store the data like this. But what happens when we have 10,000 rolls?

```{r}
more_rolls <- rep(rolls, 1000)
table(more_rolls)
```

Instead of keeping around a vector of length 10,000, we can just keep 10 values, without losing any information.

This example also highlights the fact that, graphically, two distributions can be identical even if they are of very different lengths.

```{r, echo = FALSE}
rolls_p <- tibble(value = rolls) %>% 
  ggplot(aes(value)) +
    geom_bar(aes(y = after_stat(count))) +
    labs(title = "Distribution of 10 Rolls",
         y = "Count") 


more_rolls_p <- tibble(value = more_rolls) %>% 
  ggplot(aes(value)) +
    geom_bar(aes(y = after_stat(count))) +
    labs(title = "Distribution of 10,000 Rolls",
         y = NULL)


rolls_p + more_rolls_p
```

The two vectors --- `rolls` and `more_rolls` --- have the exact same shape because even though of different lengths, these values were drawn from the same distribution. *The total count for each value does not matter. What matters is the relative proportions.*

Since shape is what matters, we often "normalize" distributions so that the sum of the counts equals one, where y-axis shows the percentages of total. Example:

```{r, echo = FALSE}
rolls_p <- tibble(value = rolls) %>% 
  ggplot(aes(x = value)) +
    geom_bar(aes(y = after_stat(count/sum(count)))) +
    labs(title = "Distribution of 10 Rolls",
         y = "Percentage") +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1))

more_rolls_p <- tibble(value = more_rolls) %>% 
  ggplot(aes(value)) +
    geom_bar(aes(y = after_stat(count/sum(count)))) +
    labs(title = "Distribution of 10,000 Rolls",
         y = NULL) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1))

rolls_p + more_rolls_p
```

### More distribution junk from Wrangling



```{r, echo = FALSE}
set.seed(5)
```

The function `rnorm()` (spoken as "r-norm") returns draws from a normal distribution. `rnorm()` has three arguments: `n`, `mean`, and `sd`. `n` corresponds to the number of draws, `mean` and `sd` are the $\mu$ and $\sigma$ of the distribution from which we want to draw.  Again, imagine an urn filled with beads. Each bead has a number written on it. If the distribution is standard normal, then we can draw 10 beads from the urn by running: 

```{r}
rnorm(10)
```

These 10 draws come from a distribution with the default `mean` of 0 and the default `sd` of 1. What if we create a histogram of the values?

```{r}
tibble(value = rnorm(10)) %>% 
  ggplot(aes(x = value)) + 
    geom_histogram(bins = 10)
```

As you can see, it is not as symmetrical as the one displayed above. This is not surprising! If you just draw 10 beads from the urn, you can not possibly have a very good sense of what all the numbers on all the beads in the urn look like. What if we draw 100 values? What about 100,000?

```{r}
tibble(value = rnorm(100)) %>% 
  ggplot(aes(x = value)) +
    geom_histogram(bins = 10)
```

```{r}
tibble(value = rnorm(100000)) %>% 
  ggplot(aes(x = value)) +
    geom_histogram(bins = 1000)
```

Now it's looking a lot more similar to the "truth", although still  imperfect. 

Now, let's compare normal distributions with varying means and standard deviations, which can be set using the mean and sd arguments included with the function. 

```{r}
tibble(rnorm_5_1 = rnorm(n = 1000, mean = 5, sd = 1), 
       rnorm_0_3 = rnorm(n = 1000, mean = 0, sd = 3),
       rnorm_0_1 = rnorm(n = 1000, mean = 0, sd = 1)) %>%
  pivot_longer(cols = everything(), 
               names_to = "distribution", 
               values_to = "value") %>% 
  ggplot(aes(x = value, fill = distribution)) +
    geom_density(alpha = 0.5) +
    labs(title = "Comparison of Normal Distributions with Differing Mean and Standard Deviation Values", 
         fill = "Distribution",
         x = "Value",
         y = "Density")
```




### Working with draws

<!-- DK: Should I be working with tibbles instead of vectors? -->

Once we have a vector of draws, we can examine various aspects of the distribution. Examples:

```{r}
draws <- rnorm(100, mean = 2, sd = 1)
```

This is a case in which there is no distinction between the *true distribution* and the *estimated distribution*. We know, by assumption, what the truth is.

Even though we know, because we wrote the code, that the draws come from a normal distribution with a mean of 2 and a standard deviation of 1, the calculated results will not match those values exactly because the draws themselves are random. 

```{r}
mean(draws)
sd(draws)
```

Note that we are more likely to use the median and the mad to summarize a distribution. In this case, they are very similar to the mean and standard deviation.

```{r}
median(draws)
mad(draws)
```


In practice, we will not know the exact distribution which generates our data. (If we did know, then estimation would not be necessary.) The inherent randomness of the world means that calculated statistics will not match the underlying truth perfectly. But the more data that we collect, the closer the match will be.

In addition to the mean and standard deviation of the draws, we will often be interested in various quantiles of the distribution, most commonly because we want to create intervals which cover a specified portion of the draws. Examples:

```{r}
quantile(draws, probs = c(0.25, 0.75))
quantile(draws, probs = c(0.05, 0.95))
quantile(draws, probs = c(0.025, 0.975))
```

Note that these draws come from a distribution which is centered around 2 rather than 0. There is nothing intrinsically special about any of these ranges. They are mere convention, especially the 95% interval.

<!-- DK: Discuss confidence intervals. -->

Note how cavalier we are in sometimes using the word "distribution" and sometimes the word "draws." These are two different things! The distribution is the underlying reality, which we will only know for certain when we create it ourselves, as in this example. The draws are a vector of numbers which, we assume, are "drawn" from some underlying distribution which, in general, we do not know. 

By assumption, we can analyze the draws to make inferences about the distribution.

Although distributions (and the draws therefrom) are complex, we can often treat them in the same way that we treat simple numbers. For example, we can add two distributions together.

```{r}
n <- 100000
tibble(Normal = rnorm(n, mean = 1),
       Uniform = runif(n, min = 2, max = 3),
       Combined = Normal + Uniform) %>% 
  pivot_longer(cols = everything(),
               names_to = "Distribution",
               values_to = "draw") %>% 
  ggplot(aes(x = draw, fill = Distribution)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   alpha = 0.5, 
                   bins = 100, 
                   position = "identity") +
    labs(title = "Two Distributions and Their Sum",
         subtitle = "You can sum distributions just like you sum numbers",
         x = "Value",
         y = "Probability")
```

Drawing from a distribution also allows us to answer questions via *simulation.* For example, imagine that A and B are both flipping fair coins. A flips the coin 3 times. B flips the coin 6 times. What is the probability that A flips more heads than B?

It is obvious that B will win this game more often than A. It is also obvious that A will win some of the  time. But in order to estimate the chances of A winning, we can simply simulate playing the game 1,000 times.


```{r}
set.seed(56)
games <- 1000 

tibble(A_heads = rbinom(n = games, size = 3, prob = 0.5),
       B_heads = rbinom(n = games, size = 6, prob = 0.5)) %>% 
  mutate(A_wins = if_else(A_heads > B_heads, 1, 0)) %>% 
  summarize(A_chances = mean(A_wins))
```

A has about a 9% chance of winning the game.

In data science, the most important kind of distribution is a *probability distribution*, a concept which we will introduce in Chapter \@ref(probability).

<!-- DK: Could do more here, like look at prediction games, betting and so on. -->







